{
  "id": "2e80e619-11e1-4576-a3c0-6ceb1c62f9c6",
  "data": {
    "nodes": [
      {
        "data": {
          "description": "Create a prompt template with dynamic variables.",
          "display_name": "Prompt",
          "id": "Prompt-D4Tlk",
          "node": {
            "base_classes": [
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {
              "template": []
            },
            "description": "Create a prompt template with dynamic variables.",
            "display_name": "Prompt",
            "documentation": "",
            "edited": false,
            "field_order": [
              "template"
            ],
            "frozen": false,
            "icon": "prompts",
            "legacy": false,
            "metadata": {},
            "output_types": [],
            "outputs": [
              {
                "cache": true,
                "display_name": "Prompt Message",
                "method": "build_prompt",
                "name": "prompt",
                "selected": "Message",
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n"
              },
              "template": {
                "_input_type": "PromptInput",
                "advanced": false,
                "display_name": "Template",
                "dynamic": false,
                "info": "",
                "list": false,
                "load_from_db": false,
                "name": "template",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "type": "prompt",
                "value": "Answer the user as if you were a GenAI expert, enthusiastic about helping them get started building something fresh."
              }
            },
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "Prompt"
        },
        "dragging": false,
        "height": 260,
        "id": "Prompt-D4Tlk",
        "position": {
          "x": 690.2015147036818,
          "y": 1018.5443911764344
        },
        "positionAbsolute": {
          "x": 690.2015147036818,
          "y": 1018.5443911764344
        },
        "selected": false,
        "type": "genericNode",
        "width": 320
      },
      {
        "data": {
          "id": "undefined-apGdI",
          "node": {
            "description": "## ðŸ“– README\n\n# SocialFlow Analytics Platform\n\n## Overview\nSocialFlow Analytics Platform combines Langflow's workflow automation with DataStax Astra DB to analyze social media engagement metrics. This solution helps content creators and social media managers make data-driven decisions through automated analysis of post performance across different content types.\n\n## Problem Statement\nSocial media managers need data-driven insights to optimize their content strategy. This platform addresses this need by automating engagement analysis and providing AI-powered insights through mock data generation and analysis.\n\n## Technical Components\n\n### MockDataGeneratorNode\nGenerates realistic social media engagement data with configurable parameters for:\n- Multiple post types (carousel, reel, static)\n- Engagement rates and patterns\n- Time-distributed posts\n- Randomized engagement metrics\n\n### AstraDBQueryNode\nProcesses and analyzes engagement data by:\n- Querying metrics from Astra DB\n- Calculating engagement rates\n- Filtering by post type\n- Generating performance insights\n\n## Setup Instructions\n\n### Prerequisites\n- Python 3.8+\n- Langflow installation\n- DataStax Astra DB account\n- Required packages: `pip install cassandra-driver langflow`\n\n### Configuration Steps\n1. Create Astra DB account and database\n2. Download secure connect bundle\n3. Generate application token\n4. Set environment variables:\n```bash\nexport ASTRA_DB_SECURE_BUNDLE_PATH=\"path/to/secure-connect-bundle.zip\"\nexport ASTRA_DB_APPLICATION_TOKEN=\"path/to/token.json\"\n```\n\n## Usage Example\n```python\n# Generate mock data\ngenerator = MockDataGeneratorNode()\ngenerator.num_posts = \"100\"\ngenerator.date_range = \"30\"\nresult = generator.build_output()\n\n# Query analytics\nquery = AstraDBQueryNode()\nquery.post_types = [\"carousel\", \"reel\", \"static\"]\nquery.time_range = \"30\"\nanalytics = query.build_output()\n```\n\n## Data Schema\n```sql\nCREATE TABLE social_media_analytics.posts (\n    post_id text PRIMARY KEY,\n    post_type text,\n    posted_at timestamp,\n    likes int,\n    shares int,\n    comments int\n)\n```\n\n## Features\n- Mock data generation for testing\n- Engagement metrics analysis\n- Post type performance comparison\n- GPT-powered insights\n- Scalable data storage\n- Custom component architecture\n\n## Best Practices\n- Regular data backups\n- Monitor database performance\n- Validate GPT-generated insights\n- Regular component testing\n\n## Support\nFor assistance:\n- Create GitHub issues\n- Check documentation\n- Contact development team",
            "display_name": "Read Me",
            "documentation": "",
            "template": {
              "backgroundColor": "neutral"
            }
          }
        },
        "dragging": false,
        "height": 250,
        "id": "undefined-apGdI",
        "position": {
          "x": -1983.1192483255213,
          "y": 532.8320449287015
        },
        "positionAbsolute": {
          "x": -1983.1192483255213,
          "y": 532.8320449287015
        },
        "resizing": false,
        "selected": false,
        "style": {
          "height": 250,
          "width": 600
        },
        "type": "noteNode",
        "width": 600
      },
      {
        "data": {
          "id": "note-vjMmv",
          "node": {
            "description": "### ðŸ’¡ Add your OpenAI API key here ðŸ‘‡",
            "display_name": "",
            "documentation": "",
            "template": {
              "backgroundColor": "transparent"
            }
          },
          "type": "note"
        },
        "dragging": false,
        "height": 324,
        "id": "note-vjMmv",
        "position": {
          "x": 1075.829573520873,
          "y": 657.2057655038416
        },
        "positionAbsolute": {
          "x": 1075.829573520873,
          "y": 657.2057655038416
        },
        "resizing": false,
        "selected": false,
        "style": {
          "height": 324,
          "width": 324
        },
        "type": "noteNode",
        "width": 324
      },
      {
        "data": {
          "description": "Generates text using OpenAI LLMs.",
          "display_name": "OpenAI",
          "id": "OpenAIModel-06Kop",
          "node": {
            "base_classes": [
              "LanguageModel",
              "Message"
            ],
            "beta": false,
            "conditional_paths": [],
            "custom_fields": {},
            "description": "Generates text using OpenAI LLMs.",
            "display_name": "OpenAI",
            "documentation": "",
            "edited": false,
            "field_order": [
              "input_value",
              "system_message",
              "stream",
              "max_tokens",
              "model_kwargs",
              "json_mode",
              "output_schema",
              "model_name",
              "openai_api_base",
              "api_key",
              "temperature",
              "seed",
              "output_parser"
            ],
            "frozen": false,
            "icon": "OpenAI",
            "legacy": false,
            "metadata": {},
            "output_types": [],
            "outputs": [
              {
                "cache": true,
                "display_name": "Text",
                "method": "text_response",
                "name": "text_output",
                "required_inputs": [],
                "selected": "Message",
                "types": [
                  "Message"
                ],
                "value": "__UNDEFINED__"
              },
              {
                "cache": true,
                "display_name": "Language Model",
                "method": "build_model",
                "name": "model_output",
                "required_inputs": [],
                "selected": "LanguageModel",
                "types": [
                  "LanguageModel"
                ],
                "value": "__UNDEFINED__"
              }
            ],
            "pinned": false,
            "template": {
              "_type": "Component",
              "api_key": {
                "_input_type": "SecretStrInput",
                "advanced": false,
                "display_name": "OpenAI API Key",
                "dynamic": false,
                "info": "The OpenAI API Key to use for the OpenAI model.",
                "input_types": [
                  "Message"
                ],
                "load_from_db": false,
                "name": "api_key",
                "password": true,
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "type": "str",
                "value": ""
              },
              "code": {
                "advanced": true,
                "dynamic": true,
                "fileTypes": [],
                "file_path": "",
                "info": "",
                "list": false,
                "load_from_db": false,
                "multiline": true,
                "name": "code",
                "password": false,
                "placeholder": "",
                "required": true,
                "show": true,
                "title_case": false,
                "type": "code",
                "value": "import operator\nfrom functools import reduce\n\nfrom langchain_openai import ChatOpenAI\nfrom pydantic.v1 import SecretStr\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.base.models.openai_constants import OPENAI_MODEL_NAMES\nfrom langflow.field_typing import LanguageModel\nfrom langflow.field_typing.range_spec import RangeSpec\nfrom langflow.inputs import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, SecretStrInput, StrInput\nfrom langflow.inputs.inputs import HandleInput\n\n\nclass OpenAIModelComponent(LCModelComponent):\n    display_name = \"OpenAI\"\n    description = \"Generates text using OpenAI LLMs.\"\n    icon = \"OpenAI\"\n    name = \"OpenAIModel\"\n\n    inputs = [\n        *LCModelComponent._base_inputs,\n        IntInput(\n            name=\"max_tokens\",\n            display_name=\"Max Tokens\",\n            advanced=True,\n            info=\"The maximum number of tokens to generate. Set to 0 for unlimited tokens.\",\n            range_spec=RangeSpec(min=0, max=128000),\n        ),\n        DictInput(\n            name=\"model_kwargs\",\n            display_name=\"Model Kwargs\",\n            advanced=True,\n            info=\"Additional keyword arguments to pass to the model.\",\n        ),\n        BoolInput(\n            name=\"json_mode\",\n            display_name=\"JSON Mode\",\n            advanced=True,\n            info=\"If True, it will output JSON regardless of passing a schema.\",\n        ),\n        DictInput(\n            name=\"output_schema\",\n            is_list=True,\n            display_name=\"Schema\",\n            advanced=True,\n            info=\"The schema for the Output of the model. \"\n            \"You must pass the word JSON in the prompt. \"\n            \"If left blank, JSON mode will be disabled. [DEPRECATED]\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            advanced=False,\n            options=OPENAI_MODEL_NAMES,\n            value=OPENAI_MODEL_NAMES[0],\n        ),\n        StrInput(\n            name=\"openai_api_base\",\n            display_name=\"OpenAI API Base\",\n            advanced=True,\n            info=\"The base URL of the OpenAI API. \"\n            \"Defaults to https://api.openai.com/v1. \"\n            \"You can change this to use other APIs like JinaChat, LocalAI and Prem.\",\n        ),\n        SecretStrInput(\n            name=\"api_key\",\n            display_name=\"OpenAI API Key\",\n            info=\"The OpenAI API Key to use for the OpenAI model.\",\n            advanced=False,\n            value=\"OPENAI_API_KEY\",\n        ),\n        FloatInput(name=\"temperature\", display_name=\"Temperature\", value=0.1),\n        IntInput(\n            name=\"seed\",\n            display_name=\"Seed\",\n            info=\"The seed controls the reproducibility of the job.\",\n            advanced=True,\n            value=1,\n        ),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # self.output_schema is a list of dictionaries\n        # let's convert it to a dictionary\n        output_schema_dict: dict[str, str] = reduce(operator.ior, self.output_schema or {}, {})\n        openai_api_key = self.api_key\n        temperature = self.temperature\n        model_name: str = self.model_name\n        max_tokens = self.max_tokens\n        model_kwargs = self.model_kwargs or {}\n        openai_api_base = self.openai_api_base or \"https://api.openai.com/v1\"\n        json_mode = bool(output_schema_dict) or self.json_mode\n        seed = self.seed\n\n        api_key = SecretStr(openai_api_key).get_secret_value() if openai_api_key else None\n        output = ChatOpenAI(\n            max_tokens=max_tokens or None,\n            model_kwargs=model_kwargs,\n            model=model_name,\n            base_url=openai_api_base,\n            api_key=api_key,\n            temperature=temperature if temperature is not None else 0.1,\n            seed=seed,\n        )\n        if json_mode:\n            if output_schema_dict:\n                output = output.with_structured_output(schema=output_schema_dict, method=\"json_mode\")\n            else:\n                output = output.bind(response_format={\"type\": \"json_object\"})\n\n        return output\n\n    def _get_exception_message(self, e: Exception):\n        \"\"\"Get a message from an OpenAI exception.\n\n        Args:\n            e (Exception): The exception to get the message from.\n\n        Returns:\n            str: The message from the exception.\n        \"\"\"\n        try:\n            from openai import BadRequestError\n        except ImportError:\n            return None\n        if isinstance(e, BadRequestError):\n            message = e.body.get(\"message\")\n            if message:\n                return message\n        return None\n"
              },
              "input_value": {
                "_input_type": "MessageInput",
                "advanced": false,
                "display_name": "Input",
                "dynamic": false,
                "info": "",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "input_value",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "json_mode": {
                "_input_type": "BoolInput",
                "advanced": true,
                "display_name": "JSON Mode",
                "dynamic": false,
                "info": "If True, it will output JSON regardless of passing a schema.",
                "list": false,
                "name": "json_mode",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "max_tokens": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Max Tokens",
                "dynamic": false,
                "info": "The maximum number of tokens to generate. Set to 0 for unlimited tokens.",
                "list": false,
                "name": "max_tokens",
                "placeholder": "",
                "range_spec": {
                  "max": 128000,
                  "min": 0,
                  "step": 0.1,
                  "step_type": "float"
                },
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": ""
              },
              "model_kwargs": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Model Kwargs",
                "dynamic": false,
                "info": "Additional keyword arguments to pass to the model.",
                "list": false,
                "name": "model_kwargs",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "model_name": {
                "_input_type": "DropdownInput",
                "advanced": false,
                "combobox": false,
                "display_name": "Model Name",
                "dynamic": false,
                "info": "",
                "name": "model_name",
                "options": [
                  "gpt-4o-mini",
                  "gpt-4o",
                  "gpt-4-turbo",
                  "gpt-4-turbo-preview",
                  "gpt-4",
                  "gpt-3.5-turbo",
                  "gpt-3.5-turbo-0125"
                ],
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": "gpt-4o"
              },
              "openai_api_base": {
                "_input_type": "StrInput",
                "advanced": true,
                "display_name": "OpenAI API Base",
                "dynamic": false,
                "info": "The base URL of the OpenAI API. Defaults to https://api.openai.com/v1. You can change this to use other APIs like JinaChat, LocalAI and Prem.",
                "list": false,
                "load_from_db": false,
                "name": "openai_api_base",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "output_parser": {
                "_input_type": "HandleInput",
                "advanced": true,
                "display_name": "Output Parser",
                "dynamic": false,
                "info": "The parser to use to parse the output of the model",
                "input_types": [
                  "OutputParser"
                ],
                "list": false,
                "name": "output_parser",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "other",
                "value": ""
              },
              "output_schema": {
                "_input_type": "DictInput",
                "advanced": true,
                "display_name": "Schema",
                "dynamic": false,
                "info": "The schema for the Output of the model. You must pass the word JSON in the prompt. If left blank, JSON mode will be disabled. [DEPRECATED]",
                "list": true,
                "name": "output_schema",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_input": true,
                "type": "dict",
                "value": {}
              },
              "seed": {
                "_input_type": "IntInput",
                "advanced": true,
                "display_name": "Seed",
                "dynamic": false,
                "info": "The seed controls the reproducibility of the job.",
                "list": false,
                "name": "seed",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "int",
                "value": 1
              },
              "stream": {
                "_input_type": "BoolInput",
                "advanced": false,
                "display_name": "Stream",
                "dynamic": false,
                "info": "Stream the response from the model. Streaming works only in Chat.",
                "list": false,
                "name": "stream",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "bool",
                "value": false
              },
              "system_message": {
                "_input_type": "MessageTextInput",
                "advanced": false,
                "display_name": "System Message",
                "dynamic": false,
                "info": "System message to pass to the model.",
                "input_types": [
                  "Message"
                ],
                "list": false,
                "load_from_db": false,
                "name": "system_message",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "type": "str",
                "value": ""
              },
              "temperature": {
                "_input_type": "FloatInput",
                "advanced": false,
                "display_name": "Temperature",
                "dynamic": false,
                "info": "",
                "list": false,
                "name": "temperature",
                "placeholder": "",
                "required": false,
                "show": true,
                "title_case": false,
                "trace_as_metadata": true,
                "type": "float",
                "value": 0.1
              }
            },
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "OpenAIModel"
        },
        "dragging": false,
        "height": 672,
        "id": "OpenAIModel-06Kop",
        "position": {
          "x": 1081.0157946607428,
          "y": 707.3740542546418
        },
        "positionAbsolute": {
          "x": 1081.0157946607428,
          "y": 707.3740542546418
        },
        "selected": false,
        "type": "genericNode",
        "width": 320
      },
      {
        "id": "CustomComponent-RxJSz",
        "type": "genericNode",
        "position": {
          "x": -451.0069005366872,
          "y": 723.5197855604027
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "secure_connect_bundle": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "trace_as_input": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "secure_connect_bundle",
                "value": "path-to-your-secure-connect-bundle",
                "display_name": "Secure Connect Bundle",
                "advanced": false,
                "input_types": [
                  "Data"
                ],
                "dynamic": false,
                "info": "Upload your Astra DB secure connect bundle (.zip)",
                "title_case": false,
                "type": "other",
                "_input_type": "DataInput"
              },
              "token_file": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "trace_as_input": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "token_file",
                "value": "path-to-your-token-json-file",
                "display_name": "Token File",
                "advanced": false,
                "input_types": [
                  "Data"
                ],
                "dynamic": false,
                "info": "Upload your Astra DB token file (.json)",
                "title_case": false,
                "type": "other",
                "_input_type": "DataInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, DataInput, Output\nfrom langflow.schema import Data\nfrom cassandra.cluster import Cluster\nfrom cassandra.auth import PlainTextAuthProvider\nimport json\nfrom datetime import datetime\nfrom typing import Optional\n\nclass AstraDBQueryNode(Component):\n    display_name = \"Astra DB Query\"\n    description = \"Queries social media analytics data from DataStax Astra DB\"\n    documentation = \"https://docs.datastax.com/en/developer/python-driver/latest/\"\n    icon = \"database\"\n    name = \"AstraDBQueryNode\"\n\n    inputs = [\n        DataInput(\n            name=\"secure_connect_bundle\",\n            display_name=\"Secure Connect Bundle\",\n            info=\"Upload your Astra DB secure connect bundle (.zip)\",\n            value=\"path-to-your-secure-connect-bundle\",\n            required=True,\n        ),\n        DataInput(\n            name=\"token_file\",\n            display_name=\"Token File\",\n            info=\"Upload your Astra DB token file (.json)\",\n            value=\"path-to-your-token-json-file\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"post_types\",\n            display_name=\"Post Types\",\n            info=\"Filter by post types (carousel, reel, static or all)\",\n            value=\"all\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"time_range\",\n            display_name=\"Time Range\",\n            info=\"Analysis time range in days (e.g., 30)\",\n            value=\"30\",\n            required=False,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Query Results\", name=\"output\", method=\"build_output\"),\n    ]\n    \n    \n    def create_index_if_not_exists(self, session):\n        \"\"\"Create necessary indexes if they don't exist\"\"\"\n        try:\n            # Create index on posted_at\n            session.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS idx_posted_at \n                ON social_media_analytics.posts (posted_at)\n            \"\"\")\n            \n            # Create index on post_type\n            session.execute(\"\"\"\n                CREATE INDEX IF NOT EXISTS idx_post_type \n                ON social_media_analytics.posts (post_type)\n            \"\"\")\n            \n        except Exception as e:\n            # Log the error but continue execution\n            self.log(f\"Warning: Could not create indexes: {str(e)}\")\n    \n\n    def process_query(self, session, post_types: str, time_range: int) -> list:\n        \"\"\"Execute query and process results\"\"\"\n        self.log(post_types)\n        # Ensure indexes exist\n        self.create_index_if_not_exists(session)\n        \n        # Calculate the date threshold based on time range\n        date_threshold = datetime.now().timestamp() - (int(time_range) * 24 * 60 * 60)\n        \n        # Build the base query\n        query = \"\"\"\n            SELECT post_type,likes, shares, comments\n            FROM social_media_analytics.posts\n            WHERE posted_at > %s\n        \"\"\"\n        params = [datetime.fromtimestamp(date_threshold)]\n        \n        # Add post type filter if specified\n        if post_types and post_types != [\"\"]:\n            placeholders = ', '.join(['%s'] * len(post_types))\n            query += f\" AND post_type IN ({placeholders}) ALLOW FILTERING\"\n            params.extend([ptype.lower() for ptype in post_types])\n            \n        # Execute query\n        rows = session.execute(query, params)\n        \n        self.log(f\"rowa: {rows}\")\n        \n        metrics = {}\n        for row in rows:\n            if row.post_type not in metrics:\n                metrics[row.post_type] = {\n                    'likes': [], 'shares': [], 'comments': []\n                }\n            metrics[row.post_type]['likes'].append(row.likes)\n            metrics[row.post_type]['shares'].append(row.shares)\n            metrics[row.post_type]['comments'].append(row.comments)\n            \n        self.log(metrics)\n            \n        results = []\n        for post_type, data in metrics.items():\n            if data['likes'] and data['shares'] and data['comments']:  # Check if there's data to avoid division by zero\n                avg_likes = sum(data['likes']) / len(data['likes'])\n                avg_shares = sum(data['shares']) / len(data['shares'])\n                avg_comments = sum(data['comments']) / len(data['comments'])\n                    \n                results.append({\n                    'post_type': post_type,\n                    'avg_likes': avg_likes,\n                    'avg_shares': avg_shares,\n                    'avg_comments': avg_comments\n                })\n            \n        return results\n\n    def build_output(self) -> Data:\n        \"\"\"Build the component output\"\"\"\n        try:\n            # Validate inputs\n            self.log(self.secure_connect_bundle)\n            if not self.secure_connect_bundle[0].data.get(\"zip_path\", None) or not self.token_file.data.get(\"file_path\", None):\n                raise ValueError(\"Secure connect bundle and token file are required\")\n\n            # Read token file\n            with open(self.token_file.data.get(\"file_path\", None), 'r') as f:\n                token_data = json.load(f)\n                client_id = token_data.get('clientId')\n                client_secret = token_data.get('secret')\n\n            if not client_id or not client_secret:\n                raise ValueError(\"Invalid token file format\")\n\n            # Set up connection\n            auth_provider = PlainTextAuthProvider(client_id, client_secret)\n            cluster = Cluster(\n                cloud={\n                    'secure_connect_bundle': self.secure_connect_bundle[0].data.get(\"zip_path\", None)\n                },\n                auth_provider=auth_provider\n            )\n            \n            post_types = [\n                post_type.strip().lower() \n                for post_type in self.post_types.split(',')\n            ] if self.post_types else [\"\"]\n            \n\n            # Connect and execute query\n            session = cluster.connect()\n            results = self.process_query(\n                session=session,\n                post_types=post_types,\n                time_range=self.time_range\n            )\n\n            # Clean up connection\n            session.shutdown()\n            cluster.shutdown()\n\n            # Prepare output data\n            output_data = {\n                'timestamp': datetime.now().isoformat(),\n                'query_parameters': {\n                    'post_types': self.post_types,\n                    'time_range': self.time_range\n                },\n                'results': results,\n                'record_count': len(results)\n            }\n\n            # Return results\n            data = Data(value=output_data)\n            self.status = data\n            return data\n\n        except Exception as e:\n            error_data = {\n                'error': str(e),\n                'timestamp': datetime.now().isoformat()\n            }\n            data = Data(value=error_data)\n            self.status = data\n            return data\n\n    def handle_error(self, error: Exception) -> Data:\n        \"\"\"Handle errors and return formatted error data\"\"\"\n        error_message = f\"Error in AstraDBQueryNode: {str(error)}\"\n        return Data(value={'error': error_message})",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "post_types": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "post_types",
                "value": "",
                "display_name": "Post Types",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Filter by post types (carousel, reel, static or all)",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "time_range": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "time_range",
                "value": "30",
                "display_name": "Time Range",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Analysis time range in days (e.g., 30)",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Queries social media analytics data from DataStax Astra DB",
            "icon": "database",
            "base_classes": [
              "Data"
            ],
            "display_name": "Custom Component",
            "documentation": "https://docs.datastax.com/en/developer/python-driver/latest/",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "output",
                "display_name": "Query Results",
                "method": "build_output",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "secure_connect_bundle",
              "token_file",
              "post_types",
              "time_range"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "CustomComponent",
          "id": "CustomComponent-RxJSz"
        },
        "selected": false,
        "width": 320,
        "height": 509,
        "dragging": false,
        "positionAbsolute": {
          "x": -451.0069005366872,
          "y": 723.5197855604027
        }
      },
      {
        "id": "File-G9ILJ",
        "type": "genericNode",
        "position": {
          "x": -492.2878956212898,
          "y": 409.36150305374974
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "path": {
                "trace_as_metadata": true,
                "file_path": "2e80e619-11e1-4576-a3c0-6ceb1c62f9c6/2025-01-09_19-05-10_social_media_performance_analysis_db-token.json",
                "fileTypes": [
                  "txt",
                  "md",
                  "mdx",
                  "csv",
                  "json",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "pdf",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "zip"
                ],
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "path",
                "value": "",
                "display_name": "DB token .json file path",
                "advanced": false,
                "dynamic": false,
                "info": "Supported file types: txt, md, mdx, csv, json, yaml, yml, xml, html, htm, pdf, docx, py, sh, sql, js, ts, tsx, zip",
                "title_case": false,
                "type": "file",
                "_input_type": "FileInput",
                "load_from_db": false
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from pathlib import Path\nfrom tempfile import NamedTemporaryFile\nfrom zipfile import ZipFile, is_zipfile\n\nfrom langflow.base.data.utils import TEXT_FILE_TYPES, parallel_load_data, parse_text_file_to_data\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, FileInput, IntInput, Output\nfrom langflow.schema import Data\n\n\nclass FileComponent(Component):\n    \"\"\"Handles loading of individual or zipped text files.\n\n    Processes multiple valid files within a zip archive if provided.\n\n    Attributes:\n        display_name: Display name of the component.\n        description: Brief component description.\n        icon: Icon to represent the component.\n        name: Identifier for the component.\n        inputs: Inputs required by the component.\n        outputs: Output of the component after processing files.\n    \"\"\"\n\n    display_name = \"File\"\n    description = \"Load a file to be used in your project.\"\n    icon = \"file-text\"\n    name = \"File\"\n\n    inputs = [\n        FileInput(\n            name=\"path\",\n            display_name=\"DB token .json file path\",\n            file_types=[*TEXT_FILE_TYPES, \"zip\"],\n            info=f\"Supported file types: {', '.join([*TEXT_FILE_TYPES, 'zip'])}\",\n        ),\n        BoolInput(\n            name=\"silent_errors\",\n            display_name=\"Silent Errors\",\n            advanced=True,\n            info=\"If true, errors will not raise an exception.\",\n        ),\n        BoolInput(\n            name=\"use_multithreading\",\n            display_name=\"Use Multithreading\",\n            advanced=True,\n            info=\"If true, parallel processing will be enabled for zip files.\",\n        ),\n        IntInput(\n            name=\"concurrency_multithreading\",\n            display_name=\"Multithreading Concurrency\",\n            advanced=True,\n            info=\"The maximum number of workers to use, if concurrency is enabled\",\n            value=4,\n        ),\n    ]\n\n    outputs = [Output(display_name=\"Data\", name=\"data\", method=\"load_file\")]\n\n    def load_file(self) -> Data:\n        \"\"\"Load and parse file(s) from a zip archive.\n\n        Raises:\n            ValueError: If no file is uploaded or file path is invalid.\n\n        Returns:\n            Data: Parsed data from file(s).\n        \"\"\"\n        # Check if the file path is provided\n        if not self.path:\n            self.log(\"File path is missing.\")\n            msg = \"Please upload a file for processing.\"\n\n            raise ValueError(msg)\n\n        resolved_path = Path(self.resolve_path(self.path))\n        try:\n            # Check if the file is a zip archive\n            if is_zipfile(resolved_path):\n                self.log(f\"Processing zip file: {resolved_path.name}.\")\n\n                return self._process_zip_file(\n                    resolved_path,\n                    silent_errors=self.silent_errors,\n                    parallel=self.use_multithreading,\n                )\n\n            self.log(f\"Processing single file: {resolved_path.name}.\")\n\n            return self._process_single_file(resolved_path, silent_errors=self.silent_errors)\n        except FileNotFoundError:\n            self.log(f\"File not found: {resolved_path.name}.\")\n\n            raise\n\n    def _process_zip_file(self, zip_path: Path, *, silent_errors: bool = False, parallel: bool = False) -> Data:\n        \"\"\"Process text files within a zip archive.\n\n        Args:\n            zip_path: Path to the zip file.\n            silent_errors: Suppresses errors if True.\n            parallel: Enables parallel processing if True.\n\n        Returns:\n            list[Data]: Combined data from all valid files.\n\n        Raises:\n            ValueError: If no valid files found in the archive.\n        \"\"\"\n        data: list[Data] = []\n        with ZipFile(zip_path, \"r\") as zip_file:\n            # Filter file names based on extensions in TEXT_FILE_TYPES and ignore hidden files\n            valid_files = [\n                name\n                for name in zip_file.namelist()\n                if (\n                    any(name.endswith(ext) for ext in TEXT_FILE_TYPES)\n                    and not name.startswith(\"__MACOSX\")\n                    and not name.startswith(\".\")\n                )\n            ]\n\n            # Raise an error if no valid files found\n            if not valid_files:\n                self.log(\"No valid files in the zip archive.\")\n\n                # Return empty data if silent_errors is True\n                if silent_errors:\n                    return data  # type: ignore[return-value]\n\n                # Raise an error if no valid files found\n                msg = \"No valid files in the zip archive.\"\n                raise ValueError(msg)\n\n            # Define a function to process each file\n            def process_file(file_name, silent_errors=silent_errors):\n                with NamedTemporaryFile(delete=False) as temp_file:\n                    temp_path = Path(temp_file.name).with_name(file_name)\n                    with zip_file.open(file_name) as file_content:\n                        temp_path.write_bytes(file_content.read())\n                try:\n                    return self._process_single_file(temp_path, silent_errors=silent_errors)\n                finally:\n                    temp_path.unlink()\n\n            # Process files in parallel if specified\n            if parallel:\n                self.log(\n                    f\"Initializing parallel Thread Pool Executor with max workers: \"\n                    f\"{self.concurrency_multithreading}.\"\n                )\n\n                # Process files in parallel\n                initial_data = parallel_load_data(\n                    valid_files,\n                    silent_errors=silent_errors,\n                    load_function=process_file,\n                    max_concurrency=self.concurrency_multithreading,\n                )\n\n                # Filter out empty data\n                data = list(filter(None, initial_data))\n            else:\n                # Sequential processing\n                data = [process_file(file_name) for file_name in valid_files]\n\n        self.log(f\"Successfully processed zip file: {zip_path.name}.\")\n\n        return data  # type: ignore[return-value]\n\n    def _process_single_file(self, file_path: Path, *, silent_errors: bool = False) -> Data:\n        \"\"\"Process a single file.\n\n        Args:\n            file_path: Path to the file.\n            silent_errors: Suppresses errors if True.\n\n        Returns:\n            Data: Parsed data from the file.\n\n        Raises:\n            ValueError: For unsupported file formats.\n        \"\"\"\n        # Check if the file type is supported\n        if not any(file_path.suffix == ext for ext in [\".\" + f for f in TEXT_FILE_TYPES]):\n            self.log(f\"Unsupported file type: {file_path.suffix}\")\n\n            # Return empty data if silent_errors is True\n            if silent_errors:\n                return Data()\n\n            msg = f\"Unsupported file type: {file_path.suffix}\"\n            raise ValueError(msg)\n\n        try:\n            # Parse the text file as appropriate\n            data = parse_text_file_to_data(str(file_path), silent_errors=silent_errors)  # type: ignore[assignment]\n            if not data:\n                data = Data()\n\n            self.log(f\"Successfully processed file: {file_path.name}.\")\n        except Exception as e:\n            self.log(f\"Error processing file {file_path.name}: {e}\")\n\n            # Return empty data if silent_errors is True\n            if not silent_errors:\n                raise\n\n            data = Data()\n        \n        return data\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "concurrency_multithreading": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "concurrency_multithreading",
                "value": 4,
                "display_name": "Multithreading Concurrency",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of workers to use, if concurrency is enabled",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "silent_errors": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "silent_errors",
                "value": false,
                "display_name": "Silent Errors",
                "advanced": true,
                "dynamic": false,
                "info": "If true, errors will not raise an exception.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "use_multithreading": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "use_multithreading",
                "value": false,
                "display_name": "Use Multithreading",
                "advanced": true,
                "dynamic": false,
                "info": "If true, parallel processing will be enabled for zip files.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              }
            },
            "description": "Load a file to be used in your project.",
            "icon": "file-text",
            "base_classes": [
              "Data"
            ],
            "display_name": "File",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "data",
                "display_name": "Data",
                "method": "load_file",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "path",
              "silent_errors",
              "use_multithreading",
              "concurrency_multithreading"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "File",
          "id": "File-G9ILJ"
        },
        "selected": false,
        "width": 320,
        "height": 232,
        "positionAbsolute": {
          "x": -492.2878956212898,
          "y": 409.36150305374974
        },
        "dragging": false
      },
      {
        "id": "File-1Y6hF",
        "type": "genericNode",
        "position": {
          "x": -1102.4698517660202,
          "y": 396.21286790586555
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "path": {
                "trace_as_metadata": true,
                "file_path": "2e80e619-11e1-4576-a3c0-6ceb1c62f9c6/2025-01-09_19-05-00_secure-connect-social-media-performance-analysis-db.zip",
                "fileTypes": [
                  "txt",
                  "md",
                  "mdx",
                  "csv",
                  "json",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "pdf",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "zip"
                ],
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "path",
                "value": "",
                "display_name": "Secure DB connect zip path",
                "advanced": false,
                "dynamic": false,
                "info": "Supported file types: txt, md, mdx, csv, json, yaml, yml, xml, html, htm, pdf, docx, py, sh, sql, js, ts, tsx, zip",
                "title_case": false,
                "type": "file",
                "_input_type": "FileInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from pathlib import Path\nfrom tempfile import NamedTemporaryFile\nfrom zipfile import ZipFile, is_zipfile\n\nfrom langflow.base.data.utils import TEXT_FILE_TYPES, parallel_load_data, parse_text_file_to_data\nfrom langflow.custom import Component\nfrom langflow.io import BoolInput, FileInput, IntInput, Output\nfrom langflow.schema import Data\n\n\nclass FileComponent(Component):\n    \"\"\"Handles loading of individual or zipped text files.\n\n    Processes multiple valid files within a zip archive if provided.\n\n    Attributes:\n        display_name: Display name of the component.\n        description: Brief component description.\n        icon: Icon to represent the component.\n        name: Identifier for the component.\n        inputs: Inputs required by the component.\n        outputs: Output of the component after processing files.\n    \"\"\"\n\n    display_name = \"File\"\n    description = \"Load a file to be used in your project.\"\n    icon = \"file-text\"\n    name = \"File\"\n\n    inputs = [\n        FileInput(\n            name=\"path\",\n            display_name=\"Secure DB connect zip path\",\n            file_types=[*TEXT_FILE_TYPES, \"zip\"],\n            info=f\"Supported file types: {', '.join([*TEXT_FILE_TYPES, 'zip'])}\",\n        ),\n        BoolInput(\n            name=\"silent_errors\",\n            display_name=\"Silent Errors\",\n            advanced=True,\n            info=\"If true, errors will not raise an exception.\",\n        ),\n        BoolInput(\n            name=\"use_multithreading\",\n            display_name=\"Use Multithreading\",\n            advanced=True,\n            info=\"If true, parallel processing will be enabled for zip files.\",\n        ),\n        IntInput(\n            name=\"concurrency_multithreading\",\n            display_name=\"Multithreading Concurrency\",\n            advanced=True,\n            info=\"The maximum number of workers to use, if concurrency is enabled\",\n            value=4,\n        ),\n    ]\n\n    outputs = [Output(display_name=\"Data\", name=\"data\", method=\"load_file\")]\n\n    def load_file(self) -> Data:\n        \"\"\"Load and parse file(s) from a zip archive.\n\n        Raises:\n            ValueError: If no file is uploaded or file path is invalid.\n\n        Returns:\n            Data: Parsed data from file(s).\n        \"\"\"\n        # Check if the file path is provided\n        if not self.path:\n            self.log(\"File path is missing.\")\n            msg = \"Please upload a file for processing.\"\n\n            raise ValueError(msg)\n\n        resolved_path = Path(self.resolve_path(self.path))\n        try:\n            # Check if the file is a zip archive\n            if is_zipfile(resolved_path):\n                self.log(f\"Processing zip file: {resolved_path.name}.\")\n\n                return self._process_zip_file(\n                    resolved_path,\n                    silent_errors=self.silent_errors,\n                    parallel=self.use_multithreading,\n                )\n\n            self.log(f\"Processing single file: {resolved_path.name}.\")\n\n            return self._process_single_file(resolved_path, silent_errors=self.silent_errors)\n        except FileNotFoundError:\n            self.log(f\"File not found: {resolved_path.name}.\")\n\n            raise\n\n    def _process_zip_file(self, zip_path: Path, *, silent_errors: bool = False, parallel: bool = False) -> Data:\n        \"\"\"Process text files within a zip archive.\n\n        Args:\n            zip_path: Path to the zip file.\n            silent_errors: Suppresses errors if True.\n            parallel: Enables parallel processing if True.\n\n        Returns:\n            list[Data]: Combined data from all valid files.\n\n        Raises:\n            ValueError: If no valid files found in the archive.\n        \"\"\"\n        data: list[Data] = []\n        with ZipFile(zip_path, \"r\") as zip_file:\n            # Filter file names based on extensions in TEXT_FILE_TYPES and ignore hidden files\n            valid_files = [\n                name\n                for name in zip_file.namelist()\n                if (\n                    any(name.endswith(ext) for ext in TEXT_FILE_TYPES)\n                    and not name.startswith(\"__MACOSX\")\n                    and not name.startswith(\".\")\n                )\n            ]\n\n            # Raise an error if no valid files found\n            if not valid_files:\n                self.log(\"No valid files in the zip archive.\")\n\n                # Return empty data if silent_errors is True\n                if silent_errors:\n                    return data  # type: ignore[return-value]\n\n                # Raise an error if no valid files found\n                msg = \"No valid files in the zip archive.\"\n                raise ValueError(msg)\n\n            # Define a function to process each file\n            def process_file(file_name, silent_errors=silent_errors):\n                with NamedTemporaryFile(delete=False) as temp_file:\n                    temp_path = Path(temp_file.name).with_name(file_name)\n                    with zip_file.open(file_name) as file_content:\n                        temp_path.write_bytes(file_content.read())\n                try:\n                    return self._process_single_file(temp_path, silent_errors=silent_errors)\n                finally:\n                    temp_path.unlink()\n\n            # Process files in parallel if specified\n            if parallel:\n                self.log(\n                    f\"Initializing parallel Thread Pool Executor with max workers: \"\n                    f\"{self.concurrency_multithreading}.\"\n                )\n\n                # Process files in parallel\n                initial_data = parallel_load_data(\n                    valid_files,\n                    silent_errors=silent_errors,\n                    load_function=process_file,\n                    max_concurrency=self.concurrency_multithreading,\n                )\n\n                # Filter out empty data\n                data = list(filter(None, initial_data))\n            else:\n                # Sequential processing\n                data = [process_file(file_name) for file_name in valid_files]\n\n        self.log(f\"Successfully processed zip file: {zip_path.name}.\")\n        \n        \n        data[0].data['zip_path'] = zip_path\n        return data  # type: ignore[return-value]\n\n    def _process_single_file(self, file_path: Path, *, silent_errors: bool = False) -> Data:\n        \"\"\"Process a single file.\n\n        Args:\n            file_path: Path to the file.\n            silent_errors: Suppresses errors if True.\n\n        Returns:\n            Data: Parsed data from the file.\n\n        Raises:\n            ValueError: For unsupported file formats.\n        \"\"\"\n        # Check if the file type is supported\n        if not any(file_path.suffix == ext for ext in [\".\" + f for f in TEXT_FILE_TYPES]):\n            self.log(f\"Unsupported file type: {file_path.suffix}\")\n\n            # Return empty data if silent_errors is True\n            if silent_errors:\n                return Data()\n\n            msg = f\"Unsupported file type: {file_path.suffix}\"\n            raise ValueError(msg)\n\n        try:\n            # Parse the text file as appropriate\n            data = parse_text_file_to_data(str(file_path), silent_errors=silent_errors)  # type: ignore[assignment]\n            if not data:\n                data = Data()\n\n            self.log(f\"Successfully processed file: {file_path.name}.\")\n        except Exception as e:\n            self.log(f\"Error processing file {file_path.name}: {e}\")\n\n            # Return empty data if silent_errors is True\n            if not silent_errors:\n                raise\n\n            data = Data()\n\n        return data\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "concurrency_multithreading": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "concurrency_multithreading",
                "value": 4,
                "display_name": "Multithreading Concurrency",
                "advanced": true,
                "dynamic": false,
                "info": "The maximum number of workers to use, if concurrency is enabled",
                "title_case": false,
                "type": "int",
                "_input_type": "IntInput"
              },
              "silent_errors": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "silent_errors",
                "value": false,
                "display_name": "Silent Errors",
                "advanced": true,
                "dynamic": false,
                "info": "If true, errors will not raise an exception.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "use_multithreading": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "use_multithreading",
                "value": false,
                "display_name": "Use Multithreading",
                "advanced": true,
                "dynamic": false,
                "info": "If true, parallel processing will be enabled for zip files.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              }
            },
            "description": "Load a file to be used in your project.",
            "icon": "file-text",
            "base_classes": [
              "Data"
            ],
            "display_name": "File",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "data",
                "display_name": "Data",
                "method": "load_file",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "path",
              "silent_errors",
              "use_multithreading",
              "concurrency_multithreading"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "File",
          "id": "File-1Y6hF"
        },
        "selected": false,
        "width": 320,
        "height": 232,
        "positionAbsolute": {
          "x": -1102.4698517660202,
          "y": 396.21286790586555
        },
        "dragging": false
      },
      {
        "id": "CustomComponent-ajfls",
        "type": "genericNode",
        "position": {
          "x": 106.91148532805266,
          "y": 725.4287441450582
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "analytics_data": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "trace_as_input": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "analytics_data",
                "value": "",
                "display_name": "Analytics Data",
                "advanced": false,
                "input_types": [
                  "Data"
                ],
                "dynamic": false,
                "info": "Social media analytics data from AstraDB Query",
                "title_case": false,
                "type": "other",
                "_input_type": "DataInput"
              },
              "additional_questions": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "additional_questions",
                "value": "",
                "display_name": "Additional Questions",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Additional questions to include in the prompt (comma-separated)",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "analysis_focus": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "analysis_focus",
                "value": "engagement,growth,strategy",
                "display_name": "Analysis Focus",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Specific aspects to focus on in the analysis",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import DataInput, MessageTextInput, Output\nfrom langflow.schema.message import Message\nimport pandas as pd\nfrom datetime import datetime\nfrom typing import List, Dict, Any\n\nclass AnalyticsProcessorNode(Component):\n    \"\"\"\n    A component that processes social media analytics data and generates\n    prompts for GPT analysis.\n    \"\"\"\n    display_name = \"Analytics Processor\"\n    description = \"Processes social media metrics and generates GPT prompts for insights\"\n    documentation = \"https://docs.langflow.org/components\"\n    icon = \"chart\"\n    name = \"AnalyticsProcessorNode\"\n\n    inputs = [\n        DataInput(\n            name=\"analytics_data\",\n            display_name=\"Analytics Data\",\n            info=\"Social media analytics data from AstraDB Query\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"additional_questions\",\n            display_name=\"Additional Questions\",\n            info=\"Additional questions to include in the prompt (comma-separated)\",\n            required=False,\n        ),\n        MessageTextInput(\n            name=\"analysis_focus\",\n            display_name=\"Analysis Focus\",\n            info=\"Specific aspects to focus on in the analysis\",\n            value=\"engagement,growth,strategy\",\n            required=False,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"GPT Prompt\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def calculate_engagement_score(self, metrics: Dict[str, float]) -> float:\n        \"\"\"Calculate an overall engagement score for a post type\"\"\"\n        return (\n            metrics['avg_likes'] + \n            metrics['avg_shares'] * 2 + \n            metrics['avg_comments'] * 3\n        ) / 6  # Normalized by dividing by 6\n\n    def format_metrics_section(self, metrics_list: List[Dict[str, Any]]) -> str:\n        \"\"\"Format the metrics section of the prompt with detailed statistics\"\"\"\n        prompt = \"Current Social Media Metrics:\\n\\n\"\n        \n        # Sort post types by engagement score\n        metrics_with_scores = []\n        for metrics in metrics_list:\n            engagement_score = self.calculate_engagement_score(metrics)\n            metrics_with_scores.append({\n                **metrics,\n                'engagement_score': engagement_score\n            })\n        \n        sorted_metrics = sorted(\n            metrics_with_scores, \n            key=lambda x: x['engagement_score'], \n            reverse=True\n        )\n\n        for metrics in sorted_metrics:\n            prompt += f\"Post Type: {metrics['post_type'].upper()}\\n\"\n            prompt += f\"- Average Likes: {metrics['avg_likes']:.1f}\\n\"\n            prompt += f\"- Average Shares: {metrics['avg_shares']:.1f}\\n\"\n            prompt += f\"- Average Comments: {metrics['avg_comments']:.1f}\\n\"\n            prompt += f\"- Overall Engagement Score: {metrics['engagement_score']:.1f}\\n\"\n            \n        prompt += \"Please display this Social Media Metrics Analysis in a table with title 'Social Media Metrics'\\n\\n\"\n            \n        return prompt\n\n    def format_analysis_questions(self, focus_areas: List[str], additional_questions: List[str]) -> str:\n        \"\"\"Format the analysis questions section of the prompt\"\"\"\n        prompt = \"Please provide a detailed analysis covering:\\n\\n\"\n\n        # Core analysis questions based on focus areas\n        if 'engagement' in focus_areas:\n            prompt += \"1. Engagement Analysis:\\n\"\n            prompt += \"   - Which post type shows the highest overall engagement?\\n\"\n            prompt += \"   - What patterns emerge in likes, shares, and comments across different post types?\\n\"\n            prompt += \"   - Are there any notable engagement rate disparities between post types?\\n\\n\"\n\n        if 'growth' in focus_areas:\n            prompt += \"2. Growth Opportunities:\\n\"\n            prompt += \"   - Which post types show the most potential for increasing audience engagement?\\n\"\n            prompt += \"   - Are there any underutilized post types that might deserve more focus?\\n\"\n            prompt += \"   - What engagement metrics could be improved for each post type?\\n\\n\"\n\n        if 'strategy' in focus_areas:\n            prompt += \"3. Strategic Recommendations:\\n\"\n            prompt += \"   - What content strategy adjustments would you recommend based on these metrics?\\n\"\n            prompt += \"   - How should the posting frequency be adjusted for different post types?\\n\"\n            prompt += \"   - What specific actions could improve engagement across all post types?\\n\\n\"\n\n        # Add additional custom questions if provided\n        if additional_questions:\n            prompt += \"4. Additional Analysis Points:\\n\"\n            for i, question in enumerate(additional_questions, 1):\n                prompt += f\"   {i}. {question.strip()}\\n\"\n            prompt += \"\\n\"\n\n        prompt += \"\\nPlease provide specific, actionable insights and recommendations based on the data provided.\"\n        return prompt\n\n    def build_output(self) -> Message:\n        \"\"\"Build the component output\"\"\"\n        try:\n            # Extract analytics data\n            analytics_data = self.analytics_data.value\n            if not analytics_data or 'results' not in analytics_data:\n                raise ValueError(\"Invalid analytics data format\")\n\n            # Process focus areas\n            focus_areas = [\n                area.strip().lower() \n                for area in self.analysis_focus.split(',')\n            ] if self.analysis_focus else ['engagement', 'growth', 'strategy']\n\n            # Process additional questions\n            additional_questions = [\n                q.strip() \n                for q in self.additional_questions.split(',')\n            ] if self.additional_questions else []\n\n            # Generate the prompt\n            prompt = (\n                f\"=== Social Media Analytics ===\\n\"\n                f\"Analysis Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n                f\"Analysis Period: {analytics_data['query_parameters']['time_range']} days\\n\\n\"\n            )\n\n            # Add metrics section\n            prompt += self.format_metrics_section(analytics_data['results'])\n\n            # Add analysis questions\n            prompt += self.format_analysis_questions(focus_areas, additional_questions)\n            \n            self.log(prompt)\n            \n            prompt_message = Message(\n                text=prompt\n            )\n\n            # # Prepare output data\n            # output_data = {\n            #     'timestamp': datetime.now().isoformat(),\n            #     'prompt': prompt,\n            #     'metadata': {\n            #         'focus_areas': focus_areas,\n            #         'additional_questions': additional_questions,\n            #         'metrics_count': len(analytics_data['results'])\n            #     }\n            # }\n\n            # Return prompt message\n            return prompt_message\n\n        except Exception as e:\n            error_data = {\n                'error': str(e),\n                'timestamp': datetime.now().isoformat()\n            }\n            return Data(value=error_data)\n\n    def handle_error(self, error: Exception) -> Data:\n        \"\"\"Handle errors and return formatted error data\"\"\"\n        error_message = f\"Error in AnalyticsProcessorNode: {str(error)}\"\n        return Data(value={'error': error_message})",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              }
            },
            "description": "Processes social media metrics and generates GPT prompts for insights",
            "icon": "chart",
            "base_classes": [
              "Message"
            ],
            "display_name": "Custom Component",
            "documentation": "https://docs.langflow.org/components",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "output",
                "display_name": "GPT Prompt",
                "method": "build_output",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "analytics_data",
              "additional_questions",
              "analysis_focus"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "CustomComponent",
          "id": "CustomComponent-ajfls"
        },
        "selected": false,
        "width": 320,
        "height": 389,
        "positionAbsolute": {
          "x": 106.91148532805266,
          "y": 725.4287441450582
        },
        "dragging": false
      },
      {
        "id": "CustomComponent-eFuHu",
        "type": "genericNode",
        "position": {
          "x": -630.6124053309454,
          "y": -189.579209388937
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "secure_connect_bundle": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "trace_as_input": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "secure_connect_bundle",
                "value": "path-to-your-secure-connect-bundle",
                "display_name": "Secure Connect Bundle",
                "advanced": false,
                "input_types": [
                  "Data"
                ],
                "dynamic": false,
                "info": "Upload your Astra DB secure connect bundle (.zip)",
                "title_case": false,
                "type": "other",
                "_input_type": "DataInput"
              },
              "token_file": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "list": false,
                "trace_as_input": true,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "token_file",
                "value": "path-to-your-token-json-file",
                "display_name": "Token File",
                "advanced": false,
                "input_types": [
                  "Data"
                ],
                "dynamic": false,
                "info": "Upload your Astra DB token file (.json)",
                "title_case": false,
                "type": "other",
                "_input_type": "DataInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.custom import Component\nfrom langflow.io import MessageTextInput, DataInput, Output\nfrom langflow.schema import Data\nfrom cassandra.cluster import Cluster\nfrom cassandra.auth import PlainTextAuthProvider\nfrom datetime import datetime, timedelta\nimport json\nimport random\nfrom typing import Optional, List, Dict\n\nclass MockDataGeneratorNode(Component):\n    display_name = \"Mock Data Generator\"\n    description = \"Generates mock social media analytics data and stores it in Astra DB\"\n    documentation = \"https://docs.datastax.com/en/developer/python-driver/latest/\"\n    icon = \"database\"\n    name = \"MockDataGeneratorNode\"\n\n    inputs = [\n        DataInput(\n            name=\"secure_connect_bundle\",\n            display_name=\"Secure Connect Bundle\",\n            info=\"Upload your Astra DB secure connect bundle (.zip)\",\n            value=\"path-to-your-secure-connect-bundle\",\n            required=True,\n        ),\n        DataInput(\n            name=\"token_file\",\n            display_name=\"Token File\",\n            info=\"Upload your Astra DB token file (.json)\",\n            value=\"path-to-your-token-json-file\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"num_posts\",\n            display_name=\"Number of Posts\",\n            info=\"Number of mock posts to generate\",\n            value=\"100\",\n            required=True,\n        ),\n        MessageTextInput(\n            name=\"date_range\",\n            display_name=\"Date Range (days)\",\n            info=\"Generate posts within this many days from now\",\n            value=\"30\",\n            required=False,\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Generator Results\", name=\"output\", method=\"build_output\"),\n    ]\n\n    def create_posts_table(self, session):\n        \"\"\"Create keyspace and table if they don't exist\"\"\"\n        # Create table\n        session.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS social_media_analytics.posts (\n                post_id text PRIMARY KEY,\n                post_type text,\n                posted_at timestamp,\n                likes int,\n                shares int,\n                comments int\n            )\n        \"\"\")\n\n    def generate_mock_data(self, num_posts: int, date_range: int) -> List[Dict]:\n        \"\"\"Generate mock social media engagement data\"\"\"\n        post_types = ['carousel', 'reel', 'static']\n        base_engagement_rates = {\n            'carousel': {'likes': 100, 'shares': 20, 'comments': 15},\n            'reel': {'likes': 150, 'shares': 30, 'comments': 25},\n            'static': {'likes': 80, 'shares': 15, 'comments': 10}\n        }\n\n        posts = []\n        current_date = datetime.now()\n        date_range_seconds = int(date_range) * 24 * 60 * 60\n\n        for i in range(int(num_posts)):\n            post_type = random.choice(post_types)\n            base_rates = base_engagement_rates[post_type]\n            \n            # Add some randomness to engagement rates\n            variance = random.uniform(0.8, 1.2)\n            time_offset = random.randint(0, date_range_seconds)\n            \n            post = {\n                'post_id': f'post_{datetime.now().timestamp()}_{i}',\n                'post_type': post_type,\n                'posted_at': current_date - timedelta(seconds=time_offset),\n                'likes': int(base_rates['likes'] * variance),\n                'shares': int(base_rates['shares'] * variance),\n                'comments': int(base_rates['comments'] * variance)\n            }\n            posts.append(post)\n\n        return posts\n\n    def insert_data(self, session, posts: List[Dict]):\n        \"\"\"Insert mock data into Astra DB\"\"\"\n        insert_query = \"\"\"\n            INSERT INTO social_media_analytics.posts \n            (post_id, post_type, posted_at, likes, shares, comments)\n            VALUES (%s, %s, %s, %s, %s, %s)\n        \"\"\"\n        \n        for post in posts:\n            session.execute(insert_query, (\n                post['post_id'],\n                post['post_type'],\n                post['posted_at'],\n                post['likes'],\n                post['shares'],\n                post['comments']\n            ))\n\n    def build_output(self) -> Data:\n        \"\"\"Build the component output\"\"\"\n        try:\n            # Validate inputs\n            if not self.secure_connect_bundle[0].data.get(\"zip_path\", None) or not self.token_file.data.get(\"file_path\", None):\n                raise ValueError(\"Secure connect bundle and token file are required\")\n\n            # Read token file\n            with open(self.token_file.data.get(\"file_path\", None), 'r') as f:\n                token_data = json.load(f)\n                client_id = token_data.get('clientId')\n                client_secret = token_data.get('secret')\n\n            if not client_id or not client_secret:\n                raise ValueError(\"Invalid token file format\")\n\n            # Set up connection\n            auth_provider = PlainTextAuthProvider(client_id, client_secret)\n            cluster = Cluster(\n                cloud={\n                    'secure_connect_bundle': self.secure_connect_bundle[0].data.get(\"zip_path\", None)\n                },\n                auth_provider=auth_provider\n            )\n\n            # Connect and create schema\n            session = cluster.connect()\n            self.create_posts_table(session)\n\n            # Generate and insert mock data\n            num_posts = int(self.num_posts or 100)\n            date_range = int(self.date_range or 30)\n            mock_posts = self.generate_mock_data(num_posts, date_range)\n            self.insert_data(session, mock_posts)\n\n            # Prepare summary statistics\n            stats_by_type = {}\n            for post in mock_posts:\n                post_type = post['post_type']\n                if post_type not in stats_by_type:\n                    stats_by_type[post_type] = {\n                        'count': 0,\n                        'total_likes': 0,\n                        'total_shares': 0,\n                        'total_comments': 0\n                    }\n                \n                stats = stats_by_type[post_type]\n                stats['count'] += 1\n                stats['total_likes'] += post['likes']\n                stats['total_shares'] += post['shares']\n                stats['total_comments'] += post['comments']\n\n            # Calculate averages\n            summary_stats = []\n            for post_type, stats in stats_by_type.items():\n                summary_stats.append({\n                    'post_type': post_type,\n                    'post_count': stats['count'],\n                    'avg_likes': stats['total_likes'] / stats['count'],\n                    'avg_shares': stats['total_shares'] / stats['count'],\n                    'avg_comments': stats['total_comments'] / stats['count']\n                })\n\n            # Clean up connection\n            session.shutdown()\n            cluster.shutdown()\n\n            # Prepare output data\n            output_data = {\n                'timestamp': datetime.now().isoformat(),\n                'generation_parameters': {\n                    'num_posts': num_posts,\n                    'date_range': date_range\n                },\n                'summary_statistics': summary_stats,\n                'total_records_generated': len(mock_posts)\n            }\n\n            return Data(value=output_data)\n\n        except Exception as e:\n            error_data = {\n                'error': str(e),\n                'timestamp': datetime.now().isoformat()\n            }\n            return Data(value=error_data)",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "date_range": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "date_range",
                "value": "30",
                "display_name": "Date Range (days)",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Generate posts within this many days from now",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "num_posts": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": true,
                "placeholder": "",
                "show": true,
                "name": "num_posts",
                "value": "100",
                "display_name": "Number of Posts",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Number of mock posts to generate",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Generates mock social media analytics data and stores it in Astra DB",
            "icon": "database",
            "base_classes": [
              "Data"
            ],
            "display_name": "Custom Component",
            "documentation": "https://docs.datastax.com/en/developer/python-driver/latest/",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Data"
                ],
                "selected": "Data",
                "name": "output",
                "display_name": "Generator Results",
                "method": "build_output",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "secure_connect_bundle",
              "token_file",
              "num_posts",
              "date_range"
            ],
            "beta": false,
            "legacy": false,
            "edited": true,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "CustomComponent",
          "id": "CustomComponent-eFuHu"
        },
        "selected": false,
        "width": 320,
        "height": 509,
        "positionAbsolute": {
          "x": -630.6124053309454,
          "y": -189.579209388937
        },
        "dragging": false
      },
      {
        "id": "ChatInput-iY3bZ",
        "type": "genericNode",
        "position": {
          "x": -1269.6640045957843,
          "y": 778.517668995939
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "files": {
                "trace_as_metadata": true,
                "file_path": "",
                "fileTypes": [
                  "txt",
                  "md",
                  "mdx",
                  "csv",
                  "json",
                  "yaml",
                  "yml",
                  "xml",
                  "html",
                  "htm",
                  "pdf",
                  "docx",
                  "py",
                  "sh",
                  "sql",
                  "js",
                  "ts",
                  "tsx",
                  "jpg",
                  "jpeg",
                  "png",
                  "bmp",
                  "image"
                ],
                "list": true,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "files",
                "value": "",
                "display_name": "Files",
                "advanced": true,
                "dynamic": false,
                "info": "Files to be sent with the message.",
                "title_case": false,
                "type": "file",
                "_input_type": "FileInput"
              },
              "background_color": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "background_color",
                "value": "",
                "display_name": "Background Color",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The background color of the icon.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "chat_icon": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "chat_icon",
                "value": "",
                "display_name": "Icon",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The icon of the message.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_USER, MESSAGE_SENDER_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        _background_color = self.background_color\n        _text_color = self.text_color\n        _icon = self.chat_icon\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n            properties={\"background_color\": _background_color, \"text_color\": _text_color, \"icon\": _icon},\n        )\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "input_value": {
                "tool_mode": false,
                "trace_as_input": true,
                "multiline": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Message to be passed as input.",
                "title_case": false,
                "type": "str",
                "_input_type": "MultilineInput"
              },
              "sender": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "Machine",
                  "User"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sender",
                "value": "User",
                "display_name": "Sender Type",
                "advanced": true,
                "dynamic": false,
                "info": "Type of sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "sender_name": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sender_name",
                "value": "User",
                "display_name": "Sender Name",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Name of the sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "session_id": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "session_id",
                "value": "",
                "display_name": "Session ID",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "should_store_message": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "should_store_message",
                "value": true,
                "display_name": "Store Messages",
                "advanced": true,
                "dynamic": false,
                "info": "Store the message in the history.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "text_color": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text_color",
                "value": "",
                "display_name": "Text Color",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The text color of the name",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Get chat inputs from the Playground.",
            "icon": "MessagesSquare",
            "base_classes": [
              "Message"
            ],
            "display_name": "Chat Input",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "message",
                "display_name": "Message",
                "method": "message_response",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "files",
              "background_color",
              "chat_icon",
              "text_color"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "ChatInput",
          "id": "ChatInput-iY3bZ"
        },
        "selected": false,
        "width": 320,
        "height": 234,
        "positionAbsolute": {
          "x": -1269.6640045957843,
          "y": 778.517668995939
        },
        "dragging": false
      },
      {
        "id": "ChatOutput-g2kPl",
        "type": "genericNode",
        "position": {
          "x": 1568.4272954808903,
          "y": 941.8349432970263
        },
        "data": {
          "node": {
            "template": {
              "_type": "Component",
              "background_color": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "background_color",
                "value": "",
                "display_name": "Background Color",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The background color of the icon.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "chat_icon": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "chat_icon",
                "value": "",
                "display_name": "Icon",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The icon of the message.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "code": {
                "type": "code",
                "required": true,
                "placeholder": "",
                "list": false,
                "show": true,
                "multiline": true,
                "value": "from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageInput, MessageTextInput, Output\nfrom langflow.schema.message import Message\nfrom langflow.schema.properties import Source\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(\n            display_name=\"Message\",\n            name=\"message\",\n            method=\"message_response\",\n        ),\n    ]\n\n    def _build_source(self, _id: str | None, display_name: str | None, source: str | None) -> Source:\n        source_dict = {}\n        if _id:\n            source_dict[\"id\"] = _id\n        if display_name:\n            source_dict[\"display_name\"] = display_name\n        if source:\n            source_dict[\"source\"] = source\n        return Source(**source_dict)\n\n    def message_response(self) -> Message:\n        _source, _icon, _display_name, _source_id = self.get_properties_from_source_component()\n        _background_color = self.background_color\n        _text_color = self.text_color\n        if self.chat_icon:\n            _icon = self.chat_icon\n        message = self.input_value if isinstance(self.input_value, Message) else Message(text=self.input_value)\n        message.sender = self.sender\n        message.sender_name = self.sender_name\n        message.session_id = self.session_id\n        message.flow_id = self.graph.flow_id if hasattr(self, \"graph\") else None\n        message.properties.source = self._build_source(_source_id, _display_name, _source)\n        message.properties.icon = _icon\n        message.properties.background_color = _background_color\n        message.properties.text_color = _text_color\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n",
                "fileTypes": [],
                "file_path": "",
                "password": false,
                "name": "code",
                "advanced": true,
                "dynamic": true,
                "info": "",
                "load_from_db": false,
                "title_case": false
              },
              "data_template": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "data_template",
                "value": "{text}",
                "display_name": "Data Template",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "input_value": {
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "input_value",
                "value": "",
                "display_name": "Text",
                "advanced": false,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Message to be passed as output.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageInput"
              },
              "sender": {
                "tool_mode": false,
                "trace_as_metadata": true,
                "options": [
                  "Machine",
                  "User"
                ],
                "combobox": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sender",
                "value": "Machine",
                "display_name": "Sender Type",
                "advanced": true,
                "dynamic": false,
                "info": "Type of sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "DropdownInput"
              },
              "sender_name": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "sender_name",
                "value": "AI",
                "display_name": "Sender Name",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "Name of the sender.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "session_id": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "session_id",
                "value": "",
                "display_name": "Session ID",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The session ID of the chat. If empty, the current session ID parameter will be used.",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              },
              "should_store_message": {
                "trace_as_metadata": true,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "should_store_message",
                "value": true,
                "display_name": "Store Messages",
                "advanced": true,
                "dynamic": false,
                "info": "Store the message in the history.",
                "title_case": false,
                "type": "bool",
                "_input_type": "BoolInput"
              },
              "text_color": {
                "tool_mode": false,
                "trace_as_input": true,
                "trace_as_metadata": true,
                "load_from_db": false,
                "list": false,
                "required": false,
                "placeholder": "",
                "show": true,
                "name": "text_color",
                "value": "",
                "display_name": "Text Color",
                "advanced": true,
                "input_types": [
                  "Message"
                ],
                "dynamic": false,
                "info": "The text color of the name",
                "title_case": false,
                "type": "str",
                "_input_type": "MessageTextInput"
              }
            },
            "description": "Display a chat message in the Playground.",
            "icon": "MessagesSquare",
            "base_classes": [
              "Message"
            ],
            "display_name": "Chat Output",
            "documentation": "",
            "custom_fields": {},
            "output_types": [],
            "pinned": false,
            "conditional_paths": [],
            "frozen": false,
            "outputs": [
              {
                "types": [
                  "Message"
                ],
                "selected": "Message",
                "name": "message",
                "display_name": "Message",
                "method": "message_response",
                "value": "__UNDEFINED__",
                "cache": true
              }
            ],
            "field_order": [
              "input_value",
              "should_store_message",
              "sender",
              "sender_name",
              "session_id",
              "data_template",
              "background_color",
              "chat_icon",
              "text_color"
            ],
            "beta": false,
            "legacy": false,
            "edited": false,
            "metadata": {},
            "tool_mode": false,
            "lf_version": "1.1.1"
          },
          "type": "ChatOutput",
          "id": "ChatOutput-g2kPl"
        },
        "selected": false,
        "width": 320,
        "height": 234,
        "positionAbsolute": {
          "x": 1568.4272954808903,
          "y": 941.8349432970263
        },
        "dragging": false
      }
    ],
    "edges": [
      {
        "animated": false,
        "className": "",
        "data": {
          "sourceHandle": {
            "dataType": "Prompt",
            "id": "Prompt-D4Tlk",
            "name": "prompt",
            "output_types": [
              "Message"
            ]
          },
          "targetHandle": {
            "fieldName": "system_message",
            "id": "OpenAIModel-06Kop",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          }
        },
        "id": "reactflow__edge-Prompt-D4Tlk{Å“dataTypeÅ“:Å“PromptÅ“,Å“idÅ“:Å“Prompt-D4TlkÅ“,Å“nameÅ“:Å“promptÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-OpenAIModel-06Kop{Å“fieldNameÅ“:Å“system_messageÅ“,Å“idÅ“:Å“OpenAIModel-06KopÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
        "source": "Prompt-D4Tlk",
        "sourceHandle": "{Å“dataTypeÅ“:Å“PromptÅ“,Å“idÅ“:Å“Prompt-D4TlkÅ“,Å“nameÅ“:Å“promptÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}",
        "target": "OpenAIModel-06Kop",
        "targetHandle": "{Å“fieldNameÅ“:Å“system_messageÅ“,Å“idÅ“:Å“OpenAIModel-06KopÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
        "selected": false
      },
      {
        "source": "File-G9ILJ",
        "sourceHandle": "{Å“dataTypeÅ“:Å“FileÅ“,Å“idÅ“:Å“File-G9ILJÅ“,Å“nameÅ“:Å“dataÅ“,Å“output_typesÅ“:[Å“DataÅ“]}",
        "target": "CustomComponent-RxJSz",
        "targetHandle": "{Å“fieldNameÅ“:Å“token_fileÅ“,Å“idÅ“:Å“CustomComponent-RxJSzÅ“,Å“inputTypesÅ“:[Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}",
        "data": {
          "targetHandle": {
            "fieldName": "token_file",
            "id": "CustomComponent-RxJSz",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "File",
            "id": "File-G9ILJ",
            "name": "data",
            "output_types": [
              "Data"
            ]
          }
        },
        "id": "reactflow__edge-File-G9ILJ{Å“dataTypeÅ“:Å“FileÅ“,Å“idÅ“:Å“File-G9ILJÅ“,Å“nameÅ“:Å“dataÅ“,Å“output_typesÅ“:[Å“DataÅ“]}-CustomComponent-RxJSz{Å“fieldNameÅ“:Å“token_fileÅ“,Å“idÅ“:Å“CustomComponent-RxJSzÅ“,Å“inputTypesÅ“:[Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}",
        "animated": false,
        "className": "",
        "selected": false
      },
      {
        "source": "File-G9ILJ",
        "sourceHandle": "{Å“dataTypeÅ“:Å“FileÅ“,Å“idÅ“:Å“File-G9ILJÅ“,Å“nameÅ“:Å“dataÅ“,Å“output_typesÅ“:[Å“DataÅ“]}",
        "target": "CustomComponent-eFuHu",
        "targetHandle": "{Å“fieldNameÅ“:Å“token_fileÅ“,Å“idÅ“:Å“CustomComponent-eFuHuÅ“,Å“inputTypesÅ“:[Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}",
        "data": {
          "targetHandle": {
            "fieldName": "token_file",
            "id": "CustomComponent-eFuHu",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "File",
            "id": "File-G9ILJ",
            "name": "data",
            "output_types": [
              "Data"
            ]
          }
        },
        "id": "reactflow__edge-File-G9ILJ{Å“dataTypeÅ“:Å“FileÅ“,Å“idÅ“:Å“File-G9ILJÅ“,Å“nameÅ“:Å“dataÅ“,Å“output_typesÅ“:[Å“DataÅ“]}-CustomComponent-eFuHu{Å“fieldNameÅ“:Å“token_fileÅ“,Å“idÅ“:Å“CustomComponent-eFuHuÅ“,Å“inputTypesÅ“:[Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}",
        "animated": false,
        "className": "",
        "selected": false
      },
      {
        "source": "File-1Y6hF",
        "sourceHandle": "{Å“dataTypeÅ“:Å“FileÅ“,Å“idÅ“:Å“File-1Y6hFÅ“,Å“nameÅ“:Å“dataÅ“,Å“output_typesÅ“:[Å“DataÅ“]}",
        "target": "CustomComponent-eFuHu",
        "targetHandle": "{Å“fieldNameÅ“:Å“secure_connect_bundleÅ“,Å“idÅ“:Å“CustomComponent-eFuHuÅ“,Å“inputTypesÅ“:[Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}",
        "data": {
          "targetHandle": {
            "fieldName": "secure_connect_bundle",
            "id": "CustomComponent-eFuHu",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "File",
            "id": "File-1Y6hF",
            "name": "data",
            "output_types": [
              "Data"
            ]
          }
        },
        "id": "reactflow__edge-File-1Y6hF{Å“dataTypeÅ“:Å“FileÅ“,Å“idÅ“:Å“File-1Y6hFÅ“,Å“nameÅ“:Å“dataÅ“,Å“output_typesÅ“:[Å“DataÅ“]}-CustomComponent-eFuHu{Å“fieldNameÅ“:Å“secure_connect_bundleÅ“,Å“idÅ“:Å“CustomComponent-eFuHuÅ“,Å“inputTypesÅ“:[Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}",
        "animated": false,
        "className": "",
        "selected": false
      },
      {
        "source": "File-1Y6hF",
        "sourceHandle": "{Å“dataTypeÅ“:Å“FileÅ“,Å“idÅ“:Å“File-1Y6hFÅ“,Å“nameÅ“:Å“dataÅ“,Å“output_typesÅ“:[Å“DataÅ“]}",
        "target": "CustomComponent-RxJSz",
        "targetHandle": "{Å“fieldNameÅ“:Å“secure_connect_bundleÅ“,Å“idÅ“:Å“CustomComponent-RxJSzÅ“,Å“inputTypesÅ“:[Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}",
        "data": {
          "targetHandle": {
            "fieldName": "secure_connect_bundle",
            "id": "CustomComponent-RxJSz",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "File",
            "id": "File-1Y6hF",
            "name": "data",
            "output_types": [
              "Data"
            ]
          }
        },
        "id": "reactflow__edge-File-1Y6hF{Å“dataTypeÅ“:Å“FileÅ“,Å“idÅ“:Å“File-1Y6hFÅ“,Å“nameÅ“:Å“dataÅ“,Å“output_typesÅ“:[Å“DataÅ“]}-CustomComponent-RxJSz{Å“fieldNameÅ“:Å“secure_connect_bundleÅ“,Å“idÅ“:Å“CustomComponent-RxJSzÅ“,Å“inputTypesÅ“:[Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}",
        "animated": false,
        "className": "",
        "selected": false
      },
      {
        "source": "CustomComponent-ajfls",
        "sourceHandle": "{Å“dataTypeÅ“:Å“CustomComponentÅ“,Å“idÅ“:Å“CustomComponent-ajflsÅ“,Å“nameÅ“:Å“outputÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}",
        "target": "OpenAIModel-06Kop",
        "targetHandle": "{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“OpenAIModel-06KopÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "OpenAIModel-06Kop",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "CustomComponent",
            "id": "CustomComponent-ajfls",
            "name": "output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-CustomComponent-ajfls{Å“dataTypeÅ“:Å“CustomComponentÅ“,Å“idÅ“:Å“CustomComponent-ajflsÅ“,Å“nameÅ“:Å“outputÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-OpenAIModel-06Kop{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“OpenAIModel-06KopÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
        "animated": false,
        "className": "",
        "selected": false
      },
      {
        "source": "ChatInput-iY3bZ",
        "sourceHandle": "{Å“dataTypeÅ“:Å“ChatInputÅ“,Å“idÅ“:Å“ChatInput-iY3bZÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}",
        "target": "CustomComponent-RxJSz",
        "targetHandle": "{Å“fieldNameÅ“:Å“post_typesÅ“,Å“idÅ“:Å“CustomComponent-RxJSzÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
        "data": {
          "targetHandle": {
            "fieldName": "post_types",
            "id": "CustomComponent-RxJSz",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "ChatInput",
            "id": "ChatInput-iY3bZ",
            "name": "message",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-ChatInput-iY3bZ{Å“dataTypeÅ“:Å“ChatInputÅ“,Å“idÅ“:Å“ChatInput-iY3bZÅ“,Å“nameÅ“:Å“messageÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-CustomComponent-RxJSz{Å“fieldNameÅ“:Å“post_typesÅ“,Å“idÅ“:Å“CustomComponent-RxJSzÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
        "animated": false,
        "className": "",
        "selected": false
      },
      {
        "source": "CustomComponent-RxJSz",
        "sourceHandle": "{Å“dataTypeÅ“:Å“CustomComponentÅ“,Å“idÅ“:Å“CustomComponent-RxJSzÅ“,Å“nameÅ“:Å“outputÅ“,Å“output_typesÅ“:[Å“DataÅ“]}",
        "target": "CustomComponent-ajfls",
        "targetHandle": "{Å“fieldNameÅ“:Å“analytics_dataÅ“,Å“idÅ“:Å“CustomComponent-ajflsÅ“,Å“inputTypesÅ“:[Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}",
        "data": {
          "targetHandle": {
            "fieldName": "analytics_data",
            "id": "CustomComponent-ajfls",
            "inputTypes": [
              "Data"
            ],
            "type": "other"
          },
          "sourceHandle": {
            "dataType": "CustomComponent",
            "id": "CustomComponent-RxJSz",
            "name": "output",
            "output_types": [
              "Data"
            ]
          }
        },
        "id": "reactflow__edge-CustomComponent-RxJSz{Å“dataTypeÅ“:Å“CustomComponentÅ“,Å“idÅ“:Å“CustomComponent-RxJSzÅ“,Å“nameÅ“:Å“outputÅ“,Å“output_typesÅ“:[Å“DataÅ“]}-CustomComponent-ajfls{Å“fieldNameÅ“:Å“analytics_dataÅ“,Å“idÅ“:Å“CustomComponent-ajflsÅ“,Å“inputTypesÅ“:[Å“DataÅ“],Å“typeÅ“:Å“otherÅ“}",
        "animated": false,
        "className": "",
        "selected": false
      },
      {
        "source": "OpenAIModel-06Kop",
        "sourceHandle": "{Å“dataTypeÅ“:Å“OpenAIModelÅ“,Å“idÅ“:Å“OpenAIModel-06KopÅ“,Å“nameÅ“:Å“text_outputÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}",
        "target": "ChatOutput-g2kPl",
        "targetHandle": "{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“ChatOutput-g2kPlÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
        "data": {
          "targetHandle": {
            "fieldName": "input_value",
            "id": "ChatOutput-g2kPl",
            "inputTypes": [
              "Message"
            ],
            "type": "str"
          },
          "sourceHandle": {
            "dataType": "OpenAIModel",
            "id": "OpenAIModel-06Kop",
            "name": "text_output",
            "output_types": [
              "Message"
            ]
          }
        },
        "id": "reactflow__edge-OpenAIModel-06Kop{Å“dataTypeÅ“:Å“OpenAIModelÅ“,Å“idÅ“:Å“OpenAIModel-06KopÅ“,Å“nameÅ“:Å“text_outputÅ“,Å“output_typesÅ“:[Å“MessageÅ“]}-ChatOutput-g2kPl{Å“fieldNameÅ“:Å“input_valueÅ“,Å“idÅ“:Å“ChatOutput-g2kPlÅ“,Å“inputTypesÅ“:[Å“MessageÅ“],Å“typeÅ“:Å“strÅ“}",
        "animated": false,
        "className": "",
        "selected": false
      }
    ],
    "viewport": {
      "x": 753.6713519584719,
      "y": -536.0560351588442,
      "zoom": 1.0106489848963296
    }
  },
  "description": "This project leverages the power of Langflow, DataStax Astra DB, and GPT to analyze and generate insights from mock social media engagement data. By simulating various post types like carousels, reels, and static images, the system fetches and processes engagement metrics to provide actionable insights.",
  "name": "social-media-performance-analytics",
  "last_tested_version": "1.1.1",
  "endpoint_name": null,
  "is_component": false
}